{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "domainclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nWLsHb4MGMO"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from random import sample\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-maZE5ZdHI84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URRuXZA9HJFD"
      },
      "source": [
        "%cd gdrive/My Drive/Deep learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS6ZTzGSMUvb"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1guCpiRTZNg"
      },
      "source": [
        "class OfficeHomeDataset(Dataset):\n",
        "    def __init__(self, data_path, domain=\"Real World\", balance=False, one_hot=False, transform=None):\n",
        "        self.transform = transform\n",
        "        self.domain = domain\n",
        "        self.balance = balance\n",
        "        self.one_hot = one_hot\n",
        "\n",
        "        # label dict\n",
        "        self.label_dict = {\"Art\": 0, \"Clipart\":1, \"Product\":2, 'Real World': 3}\n",
        "\n",
        "        # Read all file names\n",
        "        self.file_names = []\n",
        "        if self.domain is None:\n",
        "            self.n_classes = 3\n",
        "            for root, dirs, files in os.walk(data_path):\n",
        "                for filename in files:\n",
        "                    if filename == \".DS_Store\": continue\n",
        "                    elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
        "                    self.file_names.append(os.path.join(root, filename))\n",
        "        else:\n",
        "            self.n_classes = 2\n",
        "            domain_file = []\n",
        "            source_file = []\n",
        "            for root, dirs, files in os.walk(data_path):\n",
        "                if self.domain in root:\n",
        "                    for filename in files:\n",
        "                        if filename == \".DS_Store\": continue\n",
        "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
        "                        domain_file.append(os.path.join(root, filename))\n",
        "                else:\n",
        "                    for filename in files:\n",
        "                        if filename == \".DS_Store\": continue\n",
        "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
        "                        source_file.append(os.path.join(root, filename))\n",
        "            if balance:\n",
        "                self.file_names = domain_file + sample(source_file, len(domain_file))\n",
        "            else:\n",
        "                self.file_names = domain_file + source_file\n",
        "        \n",
        "        print(len(self.file_names))\n",
        "        # self.file_names = sample(self.file_names, 200)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        label = []\n",
        "        filename = self.file_names[idx]\n",
        "        img = Image.open(filename)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # print(img.shape, filename)\n",
        "        source_name = filename.split('/')[-3]\n",
        "        if self.domain is None:\n",
        "            label.append(self.label_dict[source_name])\n",
        "        else:\n",
        "            if source_name == self.domain:\n",
        "                label.append(1)\n",
        "            else: label.append(0)\n",
        "        if self.one_hot:\n",
        "            label = np.array(label)\n",
        "            label = np.eye(self.n_classes)[label]\n",
        "            label = np.float32(label)\n",
        "        else:\n",
        "            label = np.array(label)\n",
        "        # sample = {'image': img, 'label': label}\n",
        "        sample = [img, label]\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn5EEzUQMTwV"
      },
      "source": [
        "def load_training(root_path, dir, batch_size, kwargs):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize([256, 256]),\n",
        "         transforms.RandomCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor()])\n",
        "    data = OfficeHomeDataset(os.path.join(root_path, dir), transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
        "    return train_loader\n",
        "\n",
        "def load_testing(root_path, dir, batch_size, kwargs):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize([224, 224]),\n",
        "         transforms.ToTensor()])\n",
        "    data = OfficeHomeDataset(os.path.join(root_path, dir), transform=transform)\n",
        "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    return test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dykCdUoPOgsF"
      },
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9fwIap30obH"
      },
      "source": [
        "# source classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X81hkBD_W18J"
      },
      "source": [
        "class SourceClassifer(nn.Module):\n",
        "    def __init__(self, f_dim=256, n_classes=65):\n",
        "        super(SourceClassifer, self).__init__()\n",
        "\n",
        "        self.f_dim = f_dim\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Get ResNet50 model\n",
        "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=False)\n",
        "        ResNet50.fc = nn.Identity()\n",
        "        self.ResNet50 = ResNet50\n",
        "\n",
        "        self.extractor1 = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.f_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.f_dim, self.f_dim),\n",
        "            nn.BatchNorm1d(self.f_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.extractor2 = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.f_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.f_dim, self.f_dim),\n",
        "            nn.BatchNorm1d(self.f_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.extractor3 = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.f_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.f_dim, self.f_dim),\n",
        "            nn.BatchNorm1d(self.f_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.cls1 = nn.Linear(self.f_dim, self.n_classes)\n",
        "        self.cls2 = nn.Linear(self.f_dim, self.n_classes)\n",
        "        self.cls3 = nn.Linear(self.f_dim, self.n_classes)\n",
        "\n",
        "    def forward(self, data_src, label_src = 0, mark = 1, training=True):\n",
        "        \n",
        "        if training == True:\n",
        "            h1 = self.ResNet50(data_src)\n",
        "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
        "\n",
        "            if mark == 1:\n",
        "                feature1 = self.extractor1(h1)\n",
        "                pred1 = self.cls1(feature1)\n",
        "\n",
        "                cls_loss = F.cross_entropy(pred1, label_src)\n",
        "\n",
        "                return cls_loss\n",
        "\n",
        "            if mark == 2:\n",
        "                feature2 = self.extractor2(h1)\n",
        "                pred2 = self.cls2(feature2)\n",
        "\n",
        "                cls_loss = F.cross_entropy(pred2, label_src)\n",
        "\n",
        "                return cls_loss\n",
        "\n",
        "            if mark == 3:\n",
        "                feature3 = self.extractor3(h1)\n",
        "                pred3 = self.cls3(feature3)\n",
        "\n",
        "                cls_loss = F.cross_entropy(pred3, label_src)\n",
        "\n",
        "                return cls_loss\n",
        "\n",
        "        else:\n",
        "            h1 = self.ResNet50(data_src)\n",
        "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
        "\n",
        "            feature1 = self.extractor1(h1)\n",
        "            pred1 = self.cls1(feature1)\n",
        "\n",
        "            feature2 = self.extractor2(h1)\n",
        "            pred2 = self.cls2(feature2)\n",
        "\n",
        "            feature3 = self.extractor3(h1)\n",
        "            pred3 = self.cls3(feature3)\n",
        "\n",
        "            return pred1, pred2, pred3, feature1, feature2, feature3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezmDPHUguXP3"
      },
      "source": [
        "# domain classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2vYeILYVUID"
      },
      "source": [
        "class DomainClassifier(nn.Module):\n",
        "  def __init__(self, sourceClassifier, f_dim=256, n_classes=2):\n",
        "    super(DomainClassifier, self).__init__()\n",
        "    self.f_dim = f_dim\n",
        "    self.half_f_dim = self.f_dim // 2\n",
        "    self.n_classes = n_classes\n",
        "\n",
        "    self.sourceClassifier = sourceClassifier\n",
        "\n",
        "    self.domain_cls1 = nn.Sequential(\n",
        "        nn.Linear(self.f_dim, self.half_f_dim),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(self.half_f_dim, self.n_classes)\n",
        "    )\n",
        "\n",
        "    self.domain_cls2 = nn.Sequential(\n",
        "        nn.Linear(self.f_dim, self.half_f_dim),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(self.half_f_dim, self.n_classes)\n",
        "    )\n",
        "\n",
        "    self.domain_cls3 = nn.Sequential(\n",
        "        nn.Linear(self.f_dim, self.half_f_dim),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(self.half_f_dim, self.n_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, data_src, data_tgt=None, label_src=0, label_tgt=0, mark=1, training=True):\n",
        "\n",
        "    if training == True:\n",
        "        _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
        "        _, _, _, feature1_tgt, feature2_tgt, feature3_tgt = self.sourceClassifier(data_tgt, training=False)\n",
        "\n",
        "        if mark == 1:\n",
        "            logits1 = self.domain_cls1(feature1)\n",
        "            logits1_tgt = self.domain_cls1(feature1_tgt)\n",
        "            a = 1 / data_src.shape[0]\n",
        "            weights = torch.Tensor([a] * self.n_classes).to(device)\n",
        "\n",
        "            cls_loss = F.cross_entropy(logits1, label_src, weight=weights) \\\n",
        "                + F.cross_entropy(logits1_tgt, label_tgt, weight=weights)\n",
        "\n",
        "            return cls_loss\n",
        "\n",
        "        if mark == 2:\n",
        "            logits2 = self.domain_cls2(feature2)\n",
        "            logits2_tgt = self.domain_cls2(feature2_tgt)\n",
        "            a = 1 / data_src.shape[0]\n",
        "            weights = torch.Tensor([a] * self.n_classes).to(device)\n",
        "\n",
        "            cls_loss = F.cross_entropy(logits2, label_src, weight=weights) \\\n",
        "                + F.cross_entropy(logits2_tgt, label_tgt, weight=weights)\n",
        "\n",
        "            return cls_loss\n",
        "\n",
        "        if mark == 3:\n",
        "            logits3 = self.domain_cls1(feature3)\n",
        "            logits3_tgt = self.domain_cls1(feature3_tgt)\n",
        "            a = 1 / data_src.shape[0]\n",
        "            weights = torch.Tensor([a] * self.n_classes).to(device)\n",
        "\n",
        "            cls_loss = F.cross_entropy(logits3, label_src, weight=weights) \\\n",
        "                + F.cross_entropy(logits3_tgt, label_tgt, weight=weights)\n",
        "\n",
        "            return cls_loss\n",
        "\n",
        "    else:\n",
        "        _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
        "\n",
        "        logits1 = self.domain_cls1(feature1)\n",
        "        logits2 = self.domain_cls2(feature2)\n",
        "        logits3 = self.domain_cls3(feature3)\n",
        "\n",
        "        return logits1, logits2, logits3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S811kwGlN5tA"
      },
      "source": [
        "## train and test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o_eoyFzN9n6"
      },
      "source": [
        "batch_size = 16\n",
        "iteration = 6000 // 16\n",
        "epoch = 3\n",
        "cuda = True\n",
        "seed = 8\n",
        "log_interval = 20\n",
        "class_num = 65\n",
        "root_path = \"./Dataset/\"\n",
        "source1_name = \"Art\"\n",
        "source2_name = 'Clipart'\n",
        "source3_name = 'Product'\n",
        "target_name = \"Real World\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "def train(model):\n",
        "    source1_loader = load_training(root_path, source1_name, batch_size, kwargs)\n",
        "    source2_loader = load_training(root_path, source2_name, batch_size, kwargs)\n",
        "    source3_loader = load_training(root_path, source3_name, batch_size, kwargs)\n",
        "\n",
        "    target_loader = load_training(root_path, target_name, batch_size, kwargs)\n",
        "    \n",
        "    source1_iter = iter(source1_loader)\n",
        "    source2_iter = iter(source2_loader)\n",
        "    source3_iter = iter(source3_loader)\n",
        "    target_iter = iter(target_loader)\n",
        "\n",
        "    for i in range(1, iteration + 1):\n",
        "        model.train()        \n",
        "        LEARNING_RATE = 1e-4\n",
        "        optimizer = torch.optim.Adam([\n",
        "            {'params': model.domain_cls1.parameters(), 'lr': LEARNING_RATE},\n",
        "            {'params': model.domain_cls2.parameters(), 'lr': LEARNING_RATE},\n",
        "            {'params': model.domain_cls3.parameters(), 'lr': LEARNING_RATE}\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            source_data, source_label = source1_iter.next()\n",
        "        except Exception as err:\n",
        "            source1_iter = iter(source1_loader)\n",
        "            source_data, source_label = source1_iter.next()\n",
        "        \n",
        "        try:\n",
        "            target_data, target_label = target_iter.next()\n",
        "        except Exception as err:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, target_label = target_iter.next()\n",
        "\n",
        "        if cuda:\n",
        "            source_data, source_label = source_data.cuda(), source_label.reshape(-1).cuda()\n",
        "            target_data, target_label = target_data.cuda(), target_label.reshape(-1).cuda()\n",
        "            \n",
        "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
        "        target_data, target_label = Variable(target_data), Variable(target_label)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cls_loss = model(source_data, target_data, source_label, target_label, mark=1)\n",
        "        loss = cls_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source1 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "\n",
        "        try:\n",
        "            source_data, source_label = source2_iter.next()\n",
        "        except Exception as err:\n",
        "            source2_iter = iter(source2_loader)\n",
        "            source_data, source_label = source2_iter.next()\n",
        "      \n",
        "        try:\n",
        "            target_data, target_label = target_iter.next()\n",
        "        except Exception as err:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, target_label = target_iter.next()\n",
        "\n",
        "        if cuda:\n",
        "            source_data, source_label = source_data.cuda(), source_label.reshape(-1).cuda()\n",
        "            target_data, target_label = target_data.cuda(), target_label.reshape(-1).cuda()\n",
        "\n",
        "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
        "        target_data, target_label = Variable(target_data), Variable(target_label)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cls_loss = model(source_data, target_data, source_label, target_label, mark=2)\n",
        "        loss = cls_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source2 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "\n",
        "        try:\n",
        "            source_data, source_label = source3_iter.next()\n",
        "        except Exception as err:\n",
        "            source3_iter = iter(source3_loader)\n",
        "            source_data, source_label = source3_iter.next()\n",
        "\n",
        "        try:\n",
        "            target_data, target_label = target_iter.next()\n",
        "        except Exception as err:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, target_label = target_iter.next()\n",
        "\n",
        "        if cuda:\n",
        "            source_data, source_label = source_data.cuda(), source_label.reshape(-1).cuda()\n",
        "            target_data, target_label = target_data.cuda(), target_label.reshape(-1).cuda()\n",
        "\n",
        "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
        "        target_data, target_label = Variable(target_data), Variable(target_label)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cls_loss = model(source_data, target_data, source_label, target_label, mark=3)\n",
        "        loss = cls_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source3 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "            \n",
        "    return model\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    correct3 = 0\n",
        "\n",
        "    target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            if cuda:\n",
        "                data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            pred1, pred2, pred3 = model(data, training=False)\n",
        "\n",
        "            # pred1 = torch.nn.functional.softmax(pred1, dim=1)\n",
        "            # pred2 = torch.nn.functional.softmax(pred2, dim=1)\n",
        "            # pred3 = torch.nn.functional.softmax(pred3, dim=1)\n",
        "\n",
        "            pred = (pred1 + pred2 + pred3) / 3\n",
        "            test_loss += F.cross_entropy(pred, target).item()  # sum up batch loss\n",
        "            pred = pred.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred1.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct1 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred2.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct2 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred3.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct3 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        test_loss /= len(target_test_loader.dataset)\n",
        "        print(target_name, '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(target_test_loader.dataset),\n",
        "            100. * correct / len(target_test_loader.dataset)))\n",
        "        print('\\nsource1 accnum {}, source2 accnum {}，source3 accnum {}'.format(correct1, correct2, correct3))\n",
        "    return correct\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sourceClassifier = SourceClassifer(n_classes=class_num)\n",
        "    sourceClassifier.load_state_dict(torch.load(\"./results/source_classifer_ACP\"))\n",
        "    model = DomainClassifier(sourceClassifier, n_classes=2)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "    correct = 0\n",
        "    for _ in range(epoch):\n",
        "        model = train(model)\n",
        "        t_correct = test(model)\n",
        "        if t_correct > correct:\n",
        "            correct = t_correct\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print(source1_name, source2_name, source3_name, \"with\", target_name, \"%s max correct:\" % target_name, correct.item(), \"\\n\")\n",
        "    torch.save(best_model_wts, f\"./results/domain_classifer_ACP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZvvCxFfNaNe"
      },
      "source": [
        "# weight calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQwnc2kWNZwz"
      },
      "source": [
        "def get_files_length(domain):\n",
        "  file_names = []\n",
        "  data_path = os.path.join(root_path, domain)\n",
        "  print(data_path)\n",
        "  for root, dirs, files in os.walk(data_path):\n",
        "      for filename in files:\n",
        "          if filename == \".DS_Store\": continue\n",
        "          elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
        "          file_names.append(os.path.join(root, filename))\n",
        "  return(len(file_names))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = True\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "root_path = \"./Dataset\"\n",
        "sources = [\"Art\", \"Clipart\", \"Product\"]\n",
        "lens = {}\n",
        "for source in sources:\n",
        "  lens[source] = get_files_length(source)\n",
        "print(lens)\n",
        "target = \"Real World\"\n",
        "target_length = get_files_length(target)\n",
        "# sourceClassifier = SourceClassifer()\n",
        "# # sourceClassifier.load_state_dict(torch.load(\"./trained_models/source_classifer\"))\n",
        "# domainClassifier = DomainClassifier(sourceClassifier)\n",
        "# # domainClassifier.load_state_dict(torch.load(\"./trained_models/domain_classifier_Real World\"))\n",
        "# domainClassifier.to(device)\n",
        "\n",
        "domainClassifier = model\n",
        "\n",
        "hi = []\n",
        "for i, source in enumerate(sources):\n",
        "  dataloader = load_training(root_path, source, batch_size, kwargs)\n",
        "    \n",
        "  running_corrects = 0\n",
        "  correct_one = []\n",
        "  correct_zero = []\n",
        "  for data, target in dataloader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        logits1, logits2, logits3 = domainClassifier(data, training=False)\n",
        "\n",
        "        if i == 0:\n",
        "            _, preds = torch.max(logits1, 1)\n",
        "        elif i == 1:\n",
        "            _, preds = torch.max(logits2, 1)\n",
        "        elif i == 2:\n",
        "            _, preds = torch.max(logits3, 1)\n",
        "\n",
        "        correct_labbels = torch.where(preds == target, preds, -1)\n",
        "        correct_one.append((correct_labbels == 1).sum())\n",
        "        correct_zero.append((correct_labbels == 0).sum())\n",
        "        # print(preds)\n",
        "        # print(target)\n",
        "        # print(labels.shape, (preds == labels).sum(), (correct_labbels == 1).sum(), (correct_labbels == 0).sum())\n",
        "\n",
        "        # statistics\n",
        "        running_corrects += torch.sum(preds == target.data)\n",
        "  \n",
        "  all_ones = sum(correct_one)\n",
        "  all_zeros = sum(correct_zero)\n",
        "  print(\"source: \", all_ones, \" target: \", all_zeros)\n",
        "  tmp = abs(all_zeros/target_length - all_ones/lens[source])\n",
        "  print(tmp)\n",
        "  hi.append(tmp)\n",
        "  epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "  print(running_corrects, len(dataloader.dataset))\n",
        "  print('Acc: {:.4f}'.format(epoch_acc))\n",
        "\n",
        "print(hi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKYATzttQouv"
      },
      "source": [
        "print(\"hi: \", hi)\n",
        "\n",
        "K = 3\n",
        "hi = torch.Tensor(hi)\n",
        "alpha = torch.exp(-hi * K) / torch.sum(torch.exp(-hi * K))\n",
        "\n",
        "print(\"alpha: \", alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvpeJU_6SDDm"
      },
      "source": [
        "K = 3\n",
        "alpha = [0.5580, 0.2512, 0.1907]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZOIbi5HQpfQ"
      },
      "source": [
        "beta = {}\n",
        "\n",
        "for i in range(K):\n",
        "  for j in range(i, K):\n",
        "    k = f\"{i}\"+\"-\"+f\"{j}\"\n",
        "    beta[k] = min(alpha[i], alpha[j])\n",
        "\n",
        "beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orfRWT3RLuWg"
      },
      "source": [
        "# weighted aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zGIhsJ1ZPSM"
      },
      "source": [
        "## loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnSAUC864RWG"
      },
      "source": [
        "def loss_qt(output, target, mark=1, relevance=alpha, n_classes=3):\n",
        "  label = target[0]\n",
        "  weight = relevance[mark-1] / output.shape[0]\n",
        "\n",
        "  loss = F.cross_entropy(output, target) * weight\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b0MtnEgo5WN"
      },
      "source": [
        "def CORAL(source, target):\n",
        "    d = source.data.shape[1]\n",
        "\n",
        "    # source covariance\n",
        "    xm = torch.mean(source, 0, keepdim=True) - source\n",
        "    xc = xm.t() @ xm\n",
        "\n",
        "    # target covariance\n",
        "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
        "    xct = xmt.t() @ xmt\n",
        "\n",
        "    # frobenius norm between source and target\n",
        "    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n",
        "    loss = loss/(4*d*d)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dOVFaVdZN5N"
      },
      "source": [
        "def coral(source, target):\n",
        "    if len(source.shape) == 1:\n",
        "      d = source.shape[0]\n",
        "    else:\n",
        "      d = source.shape[1]  # dim vector\n",
        "\n",
        "    source_c = compute_covariance(source)\n",
        "    target_c = compute_covariance(target)\n",
        "\n",
        "    loss = torch.sum(torch.mul((source_c - target_c), (source_c - target_c)))\n",
        "\n",
        "    loss = loss / (4 * d * d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_covariance(input_data):\n",
        "    \"\"\"\n",
        "    Compute Covariance matrix of the input data\n",
        "    \"\"\"\n",
        "    n = input_data.shape[0]  # batch_size\n",
        "\n",
        "    # Check if using gpu or cpu\n",
        "    if input_data.is_cuda:\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    id_row = torch.ones(n).resize(1, n).to(device=device)\n",
        "    sum_column = torch.mm(id_row, input_data)\n",
        "    mean_column = torch.div(sum_column, n)\n",
        "    term_mul_2 = torch.mm(mean_column.t(), mean_column)\n",
        "    d_t_d = torch.mm(input_data.t(), input_data)\n",
        "    c = torch.add(d_t_d, (-1 * term_mul_2)) * 1 / (n - 1)\n",
        "\n",
        "    return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fynfb-8ZVB6"
      },
      "source": [
        "def loss_align(source1_output, source2_output, source3_output, target_output, K=3, alpha=alpha, beta=beta):\n",
        "  loss = 0.0\n",
        "\n",
        "  loss += alpha[0] * CORAL(source1_output, target_output)\n",
        "  loss += alpha[1] * CORAL(source2_output, target_output)\n",
        "  loss += alpha[2] * CORAL(source3_output, target_output)\n",
        "\n",
        "  loss += beta[\"0-1\"] * CORAL(source1_output, source2_output) / (K-1)\n",
        "  loss += beta[\"0-2\"] * CORAL(source1_output, source3_output) / (K-1)\n",
        "  loss += beta[\"1-2\"] * CORAL(source2_output, source3_output) / (K-1)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooyAfL8uY7zD"
      },
      "source": [
        "## weighted aligned source encoder, target classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlqaDieMUZ8o"
      },
      "source": [
        "class Targetclassifier(nn.Module):\n",
        "    def __init__(self, sourceClassifier, f_dim=256, c_dim=256, n_classes=65):\n",
        "        super(Targetclassifier, self).__init__()\n",
        "\n",
        "        self.sourceClassifier = sourceClassifier\n",
        "        self.f_dim = f_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Linear(self.f_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.c_dim, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Linear(self.f_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.c_dim, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.Linear(self.f_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.c_dim, self.c_dim),\n",
        "            nn.BatchNorm1d(self.c_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(self.c_dim, self.c_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.c_dim, self.n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, data_src, label_src=0, mark=1, training=True, encoding=None):\n",
        "\n",
        "        if training == True:\n",
        "\n",
        "            if mark == 1:\n",
        "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
        "                feature1 = self.encoder1(source_feature)\n",
        "                pred1 = self.cls(feature1)\n",
        "\n",
        "                loss = loss_qt(pred1, label_src, mark=1)\n",
        "\n",
        "                return loss\n",
        "\n",
        "            if mark == 2:\n",
        "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
        "                feature2 = self.encoder2(source_feature)\n",
        "                pred2 = self.cls(feature2)\n",
        "\n",
        "                loss = loss_qt(pred2, label_src, mark=2)\n",
        "\n",
        "                return loss\n",
        "\n",
        "            if mark == 3:\n",
        "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
        "                feature3 = self.encoder3(source_feature)\n",
        "                pred3 = self.cls(feature3)\n",
        "\n",
        "                loss = loss_qt(pred3, label_src, mark=3)\n",
        "\n",
        "                return loss\n",
        "\n",
        "        else:\n",
        "            if encoding is not None:\n",
        "                pred = self.cls(encoding)\n",
        "\n",
        "                return pred\n",
        "\n",
        "            else:\n",
        "\n",
        "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
        "                feature1 = self.encoder1(source_feature)\n",
        "                pred1 = self.cls(feature1)\n",
        "\n",
        "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
        "                feature2 = self.encoder2(source_feature)\n",
        "                pred2 = self.cls(feature2)\n",
        "\n",
        "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
        "                feature3 = self.encoder3(source_feature)\n",
        "                pred3 = self.cls(feature3)\n",
        "\n",
        "                return pred1, pred2, pred3, feature1, feature2, feature3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z60gC9i2ZHzv"
      },
      "source": [
        "## weighted aligned target encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtZZlcf-YzkT"
      },
      "source": [
        "class TargetEncoder(nn.Module):\n",
        "    def __init__(self, f_dim=256, n_classes=65):\n",
        "        super(TargetEncoder, self).__init__()\n",
        "        self.f_dim = f_dim\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Get ResNet50 model\n",
        "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
        "        ResNet50.fc = nn.Identity()\n",
        "        self.ResNet50 = ResNet50\n",
        "        for param in self.ResNet50.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
        "            nn.ELU(),\n",
        "            nn.Linear(1024, self.f_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(self.f_dim, self.f_dim),\n",
        "            nn.BatchNorm1d(self.f_dim),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        h1 = self.ResNet50(input_batch)\n",
        "        h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
        "        feature = self.encoder(h1)\n",
        "        return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StG9ANveZKKf"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hQaIC7v_Dyf"
      },
      "source": [
        "## data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAATJNXkZJx6"
      },
      "source": [
        "def load_training(root_path, dir, batch_size, kwargs):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize([256, 256]),\n",
        "         transforms.RandomCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor()])\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, dir), transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
        "    return train_loader\n",
        "\n",
        "def load_testing(root_path, dir, batch_size, kwargs):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize([224, 224]),\n",
        "         transforms.ToTensor()])\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, dir), transform=transform)\n",
        "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    return test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udd4T7d2dxnt"
      },
      "source": [
        "batch_size = 16\n",
        "iteration = 6000 // 16\n",
        "epoch = 3\n",
        "cuda = True\n",
        "seed = 8\n",
        "log_interval = 20\n",
        "class_num = 65\n",
        "root_path = \"./Dataset/\"\n",
        "source1_name = \"Art\"\n",
        "source2_name = 'Clipart'\n",
        "source3_name = 'Product'\n",
        "target_name = \"Real World\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "def train(model1, model2):\n",
        "    source1_loader = load_training(root_path, source1_name, batch_size, kwargs)\n",
        "    source2_loader = load_training(root_path, source2_name, batch_size, kwargs)\n",
        "    source3_loader = load_training(root_path, source3_name, batch_size, kwargs)\n",
        "    target_loader = load_training(root_path, target_name, batch_size, kwargs)\n",
        "    \n",
        "    source1_iter = iter(source1_loader)\n",
        "    source2_iter = iter(source2_loader)\n",
        "    source3_iter = iter(source3_loader)\n",
        "    target_iter = iter(target_loader)\n",
        "\n",
        "    for i in range(1, iteration + 1):\n",
        "        model1.train()\n",
        "        model2.train()          \n",
        "        LEARNING_RATE = 1e-4\n",
        "\n",
        "        # optimizer of source encoder & target classifier\n",
        "        optimizer1 = torch.optim.Adam([\n",
        "            {'params': model1.encoder1.parameters(), 'lr': LEARNING_RATE},\n",
        "            {'params': model1.encoder2.parameters(), 'lr': LEARNING_RATE},\n",
        "            {'params': model1.encoder3.parameters(), 'lr': LEARNING_RATE},\n",
        "            {'params': model1.cls.parameters(), 'lr': LEARNING_RATE}\n",
        "        ])\n",
        "\n",
        "        # optimizer of target encoder\n",
        "        optimizer2 = torch.optim.Adam([\n",
        "            {'params': model2.encoder.parameters(), 'lr': LEARNING_RATE}\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            source1_data, source1_label = source1_iter.next()\n",
        "        except Exception as err:\n",
        "            source1_iter = iter(source1_loader)\n",
        "            source1_data, source1_label = source1_iter.next()\n",
        "          \n",
        "        source1_data, source1_label = source1_data.to(device), source1_label.reshape(-1).to(device)\n",
        "        source1_data, source1_label = Variable(source1_data), Variable(source1_label)\n",
        "        optimizer1.zero_grad()\n",
        "\n",
        "        qt_loss = model1(source1_data, source1_label, mark=1)\n",
        "        loss = qt_loss\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source1 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "\n",
        "        try:\n",
        "            source2_data, source2_label = source2_iter.next()\n",
        "        except Exception as err:\n",
        "            source2_iter = iter(source2_loader)\n",
        "            source2_data, source2_label = source2_iter.next()\n",
        "          \n",
        "        source2_data, source2_label = source2_data.to(device), source2_label.reshape(-1).to(device)\n",
        "        source2_data, source2_label = Variable(source2_data), Variable(source2_label)\n",
        "        optimizer1.zero_grad()\n",
        "\n",
        "        qt_loss = model1(source2_data, source2_label, mark=1)\n",
        "        loss = qt_loss\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source2 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "\n",
        "        try:\n",
        "            source3_data, source3_label = source3_iter.next()\n",
        "        except Exception as err:\n",
        "            source3_iter = iter(source3_loader)\n",
        "            source3_data, source3_label = source3_iter.next()\n",
        "          \n",
        "        source3_data, source3_label = source3_data.to(device), source3_label.reshape(-1).to(device)\n",
        "        source3_data, source3_label = Variable(source3_data), Variable(source3_label)\n",
        "        optimizer1.zero_grad()\n",
        "\n",
        "        qt_loss = model1(source3_data, source3_label, mark=1)\n",
        "        loss = qt_loss\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "            print('Train source3 iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, 100. * i / iteration, loss.item()))\n",
        "            \n",
        "        try:\n",
        "            target_data, __ = target_iter.next()\n",
        "        except Exception as err:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, __ = target_iter.next()\n",
        "\n",
        "        target_data = target_data.to(device)\n",
        "        target_data = Variable(target_data)\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        target_feature = model2(target_data)\n",
        "\n",
        "        _, _, _, feature1, _, _ = model1(source1_data, training=False)\n",
        "        _, _, _, _, feature2, _ = model1(source2_data, training=False)\n",
        "        _, _, _, _, _, feature3 = model1(source3_data, training=False)\n",
        "        loss2 = loss_align(feature1, feature2, feature3, target_feature)\n",
        "        loss2.backward()\n",
        "        optimizer2.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "              print('Train target iter: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  i, 100. * i / iteration, loss2.item()))\n",
        "    \n",
        "    return model1, model2\n",
        "\n",
        "def test(mode11, model2):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    correct3 = 0\n",
        "\n",
        "    target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            if cuda:\n",
        "                data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            pred1, pred2, pred3, _, _, _ = model1(data, training=False)\n",
        "\n",
        "            # pred1 = torch.nn.functional.softmax(pred1, dim=1)\n",
        "            # pred2 = torch.nn.functional.softmax(pred2, dim=1)\n",
        "            # pred3 = torch.nn.functional.softmax(pred3, dim=1)\n",
        "\n",
        "            pred = (pred1 + pred2 + pred3) / 3\n",
        "            test_loss += F.cross_entropy(pred, target).item()  # sum up batch loss\n",
        "            pred = pred.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred1.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct1 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred2.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct2 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            pred = pred3.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct3 += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        test_loss /= len(target_test_loader.dataset)\n",
        "        print(target_name, '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(target_test_loader.dataset),\n",
        "            100. * correct / len(target_test_loader.dataset)))\n",
        "        print('\\nsource1 accnum {}, source2 accnum {}，source3 accnum {}'.format(correct1, correct2, correct3))\n",
        "    return correct\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sourceClassifier = SourceClassifer(n_classes=class_num)\n",
        "    sourceClassifier.load_state_dict(torch.load(\"./results/source_classifer_ACP\"))\n",
        "    for param in sourceClassifier.ResNet50.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model1 = Targetclassifier(sourceClassifier)\n",
        "    model2 = TargetEncoder()\n",
        "    \n",
        "    best_model_wts1 = copy.deepcopy(model1.state_dict())\n",
        "    best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
        "\n",
        "    if cuda:\n",
        "        model1.cuda()\n",
        "        model2.cuda()\n",
        "    correct = 0\n",
        "    for _ in range(epoch):\n",
        "        model1, model2 = train(model1, model2)\n",
        "        t_correct = test(model1, model2)\n",
        "        if t_correct > correct:\n",
        "            correct = t_correct\n",
        "            best_model_wts = copy.deepcopy(model1.state_dict())\n",
        "        print(source1_name, source2_name, source3_name, \"with\", target_name, \"%s max correct:\" % target_name, correct.item(), \"\\n\")\n",
        "    best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
        "    torch.save(best_model_wts1, f\"./results/target_classifer_ACP\")\n",
        "    torch.save(best_model_wts2, f\"./results/target_encoder_ACP\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImacC0vAs75w"
      },
      "source": [
        "# loss T->W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRRrYgIatV8d"
      },
      "source": [
        "## weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y759S9mtUhE"
      },
      "source": [
        "batch_size = 16\n",
        "iteration = 6000 // 16\n",
        "epoch = 1\n",
        "cuda = True\n",
        "seed = 8\n",
        "log_interval = 20\n",
        "class_num = 65\n",
        "root_path = \"./Dataset/\"\n",
        "source1_name = \"Art\"\n",
        "source2_name = 'Clipart'\n",
        "source3_name = 'Product'\n",
        "target_name = \"Real World\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "sourceClassifier = SourceClassifer(n_classes=class_num)\n",
        "sourceClassifier.load_state_dict(torch.load(\"./results/source_classifer_ACP\"))\n",
        "model = DomainClassifier(sourceClassifier, n_classes=2)\n",
        "model.load_state_dict(torch.load(\"./results/domain_classifer_ACP\"))\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "\n",
        "target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
        "source1_weight = []\n",
        "source2_weight = []\n",
        "source3_weight = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in target_test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        logits1, logits2, logits3 = model(data, training=False)\n",
        "        logits1 = softmax(logits1)\n",
        "        logits2 = softmax(logits2)\n",
        "        logits3 = softmax(logits3)\n",
        "\n",
        "        weights_sum = logits1[:, 1] * alpha[0] + logits2[:, 1] * alpha[1] + logits3[:, 1] * alpha[2]\n",
        "\n",
        "        w1 = logits1[:, 1] * alpha[0] / weights_sum\n",
        "        w2 = logits2[:, 1] * alpha[1] / weights_sum\n",
        "        w3 = logits3[:, 1] * alpha[2] / weights_sum\n",
        "\n",
        "        source1_weight.append(w1)\n",
        "        source2_weight.append(w2)\n",
        "        source3_weight.append(w3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beZdO9_ZmGzX"
      },
      "source": [
        "len(source2_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs2bZr68tZvI"
      },
      "source": [
        "## loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCsCcNPfs7Yh"
      },
      "source": [
        "def loss_tw(es, et, w, ind, mark=1):\n",
        "    res = 0.0\n",
        "\n",
        "    loss = nn.MSELoss()\n",
        "    res = torch.sum(w * loss(et, es))\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-pwhlFo2I4f"
      },
      "source": [
        "input = torch.randn(16, 8, requires_grad=True)\n",
        "target = torch.randn(16, 8)\n",
        "loss = loss_tw(input, target, 0)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ULRqCeMqJWO"
      },
      "source": [
        "def loss_distill(w1, w2, w3, pred1, pred2, pred3, pred):\n",
        "    loss = nn.L1Loss()\n",
        "\n",
        "    res = 0.0\n",
        "    w1 = w1.reshape(-1, 1)\n",
        "    w2 = w2.reshape(-1, 1)\n",
        "    w3 = w3.reshape(-1, 1)\n",
        "\n",
        "    phi = w1 * pred1 + w2 * pred2 + w3 * pred3\n",
        "\n",
        "    res = loss(pred, phi)\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cybCGK8Azlts"
      },
      "source": [
        "input = torch.randn(16, 8, requires_grad=True)\n",
        "target = torch.randn(16, 8)\n",
        "w = torch.randn(16)\n",
        "loss = loss_distill(w, w, w, input, input, input, target)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbsJ1KT0syhP"
      },
      "source": [
        "def loss_entropy(pred):\n",
        "    res = 0.0\n",
        "    p = F.softmax(pred, dim=-1)\n",
        "    res = -1 * torch.sum(p * F.log_softmax(pred, dim=-1)) / pred.size()[0]\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcfSGw8ywdfW"
      },
      "source": [
        "def loss_de(iteration, distill, entropy, m=0.0036):\n",
        "    res = 0.0\n",
        "    mu = min(1, m*iteration)\n",
        "\n",
        "    res = (1-mu) * distill + mu * entropy\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQA4ZJb40UZi"
      },
      "source": [
        "  ## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDU0ONPN0T-t"
      },
      "source": [
        "iteration = len(source1_weight)\n",
        "print(\"iteration: \", iteration)\n",
        "epoch = 15\n",
        "cuda = True\n",
        "seed = 8\n",
        "log_interval = 20\n",
        "class_num = 65\n",
        "batch_size = 32\n",
        "root_path = \"./Dataset/\"\n",
        "source1_name = \"Art\"\n",
        "source2_name = 'Clipart'\n",
        "source3_name = 'Product'\n",
        "target_name = \"Real World\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
        "\n",
        "def train(model1, model2, domainclassifier):\n",
        "\n",
        "    # target_test_loader = load_training(root_path, target_name, batch_size, kwargs)\n",
        "    target_iter = iter(target_test_loader)  \n",
        "\n",
        "    for param in model1.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for i in range(1, iteration + 1):\n",
        "        model1.eval()\n",
        "        model2.train()         \n",
        "        LEARNING_RATE = 1e-4\n",
        "\n",
        "        # optimizer of target encoder\n",
        "        optimizer2 = torch.optim.Adam([\n",
        "            {'params': model2.encoder.parameters(), 'lr': LEARNING_RATE}\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            target_data, __ = target_iter.next()\n",
        "        except Exception as err:\n",
        "            target_iter = iter(target_test_loader)\n",
        "            target_data, __ = target_iter.next()\n",
        "\n",
        "        target_data = target_data.to(device)\n",
        "        target_data = Variable(target_data)\n",
        "\n",
        "        pred1, pred2, pred3 = domainclassifier(target_data, training=False)\n",
        "        sc_pred1, sc_pred2, sc_pred3, _, _, _ = sourceClassifier(target_data, training=False)\n",
        "\n",
        "        logits1 = softmax(pred1)\n",
        "        logits2 = softmax(pred2)\n",
        "        logits3 = softmax(pred3)\n",
        "\n",
        "        weights_sum = logits1[:, 1] * alpha[0] + logits2[:, 1] * alpha[1] + logits3[:, 1] * alpha[2]\n",
        "\n",
        "        w1 = logits1[:, 1] * alpha[0] / weights_sum\n",
        "        w2 = logits2[:, 1] * alpha[1] / weights_sum\n",
        "        w3 = logits3[:, 1] * alpha[2] / weights_sum\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        target_feature = model2(target_data)\n",
        "        pred_target = model1(None, training=False, encoding=target_feature)\n",
        "\n",
        "        _, _, _, feature1, feature2, feature3 = model1(target_data, training=False)\n",
        "\n",
        "        loss = 0.0\n",
        "\n",
        "        loss_value_tw = loss_tw(feature1, target_feature, w1, i-1, mark=1)\n",
        "        loss_value_tw += loss_tw(feature2, target_feature, w2, i-1, mark=2)\n",
        "        loss_value_tw += loss_tw(feature3, target_feature, w3, i-1, mark=3)\n",
        "        loss_value_tw /= 3\n",
        "\n",
        "        loss_value_distill = loss_distill(w1, w2, w3, sc_pred1, sc_pred2, sc_pred3, pred_target)\n",
        "        loss_value_entropy = loss_entropy(pred_target)\n",
        "\n",
        "        loss_value_de = loss_de(i, loss_value_distill, loss_value_entropy, m=0.01)\n",
        "\n",
        "        loss = loss_value_tw + loss_value_de\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "\n",
        "        if i % log_interval == 0:\n",
        "              print('Train target iter: {} [({:.0f}%)]\\tT->WLoss: {:.6f}\\tLoss_distill: {:.6f}\\tLoss_entropy: {:.6f}\\tLoss_de: {:.6f}\\tLoss: {:.6f}'.format(\n",
        "                  i, 100. * i / iteration, loss_value_tw.item(), loss_value_distill.item(), loss_value_entropy.item(), loss_value_de.item(), loss.item()))\n",
        "    \n",
        "    return model1, model2\n",
        "\n",
        "def test(mode11, model2):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    correct3 = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            if cuda:\n",
        "                data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            feature = model2(data)\n",
        "\n",
        "            pred = model1(None, training=False, encoding=feature)\n",
        "            pred = pred.data.max(1)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        print(target_name, '\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            correct, len(target_test_loader.dataset),\n",
        "            100. * correct / len(target_test_loader.dataset)))\n",
        "        print('\\nsource1 accnum {}, source2 accnum {}，source3 accnum {}'.format(correct1, correct2, correct3))\n",
        "\n",
        "    return correct\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sourceClassifier = SourceClassifer(n_classes=class_num)\n",
        "    sourceClassifier.load_state_dict(torch.load(\"./results/source_classifer_ACP\"))\n",
        "    for param in sourceClassifier.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    domainclassifier = DomainClassifier(sourceClassifier, n_classes=2)\n",
        "    domainclassifier.load_state_dict(torch.load(\"./results/domain_classifer_ACP\"))\n",
        "\n",
        "    model1 = Targetclassifier(sourceClassifier)\n",
        "    model2 = TargetEncoder()\n",
        "\n",
        "    model1.load_state_dict(torch.load(\"./results/target_classifer_ACP\"))\n",
        "    model2.load_state_dict(torch.load(\"./results/target_encoder_ACP\"))\n",
        "    \n",
        "    best_model_wts1 = copy.deepcopy(model1.state_dict())\n",
        "    best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
        "\n",
        "    if cuda:\n",
        "        model1.cuda()\n",
        "        model2.cuda()\n",
        "        domainclassifier.cuda()\n",
        "    correct = 0\n",
        "    for _ in range(epoch):\n",
        "        model1, model2 = train(model1, model2, domainclassifier)\n",
        "        t_correct = test(model1, model2)\n",
        "        if t_correct > correct:\n",
        "            correct = t_correct\n",
        "            best_model_wts = copy.deepcopy(model1.state_dict())\n",
        "        print(source1_name, source2_name, source3_name, \"with\", target_name, \"%s max correct:\" % target_name, correct.item(), \"\\n\")\n",
        "    best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
        "    torch.save(best_model_wts1, f\"./results/target_classifer_ACP_new\")\n",
        "    torch.save(best_model_wts2, f\"./results/target_encoder_ACP_new\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBTWRhf50SNf"
      },
      "source": [
        "# Final testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAGtf48qvYZZ"
      },
      "source": [
        "batch_size = 16\n",
        "cuda = True\n",
        "seed = 8\n",
        "log_interval = 20\n",
        "class_num = 65\n",
        "root_path = \"./Dataset/\"\n",
        "target_name = \"Real World\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "def test(mode11, model2):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    correct = 0\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    correct3 = 0\n",
        "\n",
        "    target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            if cuda:\n",
        "                data, target = data.cuda(), target.reshape(-1).cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            feature = model2(data)\n",
        "\n",
        "            pred = model1(None, training=False, encoding=feature)\n",
        "            pred = pred.data.max(1)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        print(target_name, '\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            correct, len(target_test_loader.dataset),\n",
        "            100. * correct / len(target_test_loader.dataset)))\n",
        "        print('\\nsource1 accnum {}, source2 accnum {}，source3 accnum {}'.format(correct1, correct2, correct3))\n",
        "\n",
        "    return correct\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    sourceClassifier = SourceClassifer(n_classes=class_num)\n",
        "    sourceClassifier.load_state_dict(torch.load(\"./results/source_classifer_ACP\"))\n",
        "    for param in sourceClassifier.ResNet50.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model1 = Targetclassifier(sourceClassifier)\n",
        "    model2 = TargetEncoder()\n",
        "\n",
        "    model1.load_state_dict(torch.load(\"./results/target_classifer_ACP_tw\"))\n",
        "    model2.load_state_dict(torch.load(\"./results/target_encoder_ACP_tw\"))\n",
        "\n",
        "    if cuda:\n",
        "        model1.cuda()\n",
        "        model2.cuda()\n",
        "    correct = 0\n",
        "\n",
        "    t_correct = test(model1, model2)\n",
        "    \n",
        "    print(source1_name, source2_name, source3_name, \"with\", target_name, \"%s max correct:\" % target_name, t_correct.item(), \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fODMo_FDUNpj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Project: WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 10 Jiaming Xu (5247578) and Siwei Wang (5239982)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the learning goals for Deep Learning course is to reproduce a paper given by the course instructor. This blog gives clear information about our reproduce work for the paper WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation from Surbhi Aggarwal, Jogendra Nath Kundu, R. Venkatesh Babu, and Anirban Chakraborty. Our reproduction is based on the original paper, also some other papers related to Multi-source Domain Adaptation，and online resources.\n",
    "\n",
    "The paper presents a novel method for Multi-source Domain Adaptation named WAMDA which uses multiple sources to train a predictor based on their internal relevance and their relevance score related to the target. Our work is to reproduce the proposed approach on only one dataset **OfficeHome dataset** and the other two methods (Resnet and MFSAN for evaluating the effectiveness of the proposed method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sturcture\n",
    "\n",
    "WAMDA is a method that can do effective multi-source domain adaptation based on the source-target and source-source similarities. The following gives information about the basic structure.\n",
    "\n",
    "The proposed algorithm is divided into two parts. The first stage is *pre-adaptation training* , we can obtain the relevance score, feature extractor, source classifier, and domain classifier from this stage. Then, the other stage is *multi-source adaptation training* , the weighted alignment of domains is performed, classifiers and a target encoder are learned based on this weighted aligned space. The basic model structure is shown as follows: ![avatar](imgs/process.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "The original paper did experiments on three datasets(*Office-31, Office-Caltech, and Office-Home*). In addition, it uses four types of baseline(*No Adapt, Single-Source Best, Single-Source Best, and Multi-Source* to analyze the performance of MSDA methods. For our experiment, we only did experiment on *office-Home* dataset and there are only two types of baseline we have implemented: (1) *No Adapt*: Resnet and the Proposed method (2)*Multi-Source*: MFSAN and the Proposed method. Last but not the least, the implementation steps are just the same as the original paper and will be described in the next section. Due to time limitations, we only have to implement the first row and the third row of table 5 for **OfficeHomeDataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementaion\n",
    "All the experiments were done on Google Colab, and the framework we used is PyTorch. This method is achieved by these steps. First, the dataset was downloaded and transferred into a certain format. Then，we trained feature extractors, source classifiers, and a domain classifier based on the datasets. After that, we extract the relevance scores from the last step, and the scores will be used in the following steps. We also trained weighted aligned source encoders, target classifiers, and a target encoder.\n",
    "\n",
    "The following libaries are used in onr experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We export the *Office-home* dataset to Google Cloud Disk and decompress it. The Office-Home dataset has been created to evaluate domain adaptation algorithms for object recognition using deep learning. It consists of images from 4 different domains: Artistic images, Clip Art, Product images, and Real-World images. For each domain, the dataset contains images of 65 object categories found typically in Office and Home settings. The following is going to show the eg from the dataset. ![avatar](imgs/dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement a class to load the data and assign each class a different label. In addition, we also implement some settings of one-hot, transformer, balance setting which can be used according to different model requirements. The following code can show the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfficeHomeDataset(Dataset):\n",
    "    def __init__(self, data_path, domain=\"Real World\", balance=False, one_hot=False, transform=None):\n",
    "        self.transform = transform\n",
    "        self.domain = domain\n",
    "        self.balance = balance\n",
    "        self.one_hot = one_hot\n",
    "\n",
    "        # label dict\n",
    "        self.label_dict = {\"Art\": 0, \"Clipart\":1, \"Product\":2, 'Real World': 3}\n",
    "\n",
    "        # Read all file names\n",
    "        self.file_names = []\n",
    "        if self.domain is None:\n",
    "            self.n_classes = 3\n",
    "            for root, dirs, files in os.walk(data_path):\n",
    "                for filename in files:\n",
    "                    if filename == \".DS_Store\": continue\n",
    "                    elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                    self.file_names.append(os.path.join(root, filename))\n",
    "        else:\n",
    "            self.n_classes = 2\n",
    "            domain_file = []\n",
    "            source_file = []\n",
    "            for root, dirs, files in os.walk(data_path):\n",
    "                if self.domain in root:\n",
    "                    for filename in files:\n",
    "                        if filename == \".DS_Store\": continue\n",
    "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                        domain_file.append(os.path.join(root, filename))\n",
    "                else:\n",
    "                    for filename in files:\n",
    "                        if filename == \".DS_Store\": continue\n",
    "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                        source_file.append(os.path.join(root, filename))\n",
    "            if balance:\n",
    "                self.file_names = domain_file + sample(source_file, len(domain_file))\n",
    "            else:\n",
    "                self.file_names = domain_file + source_file\n",
    "        \n",
    "        print(len(self.file_names))\n",
    "        # self.file_names = sample(self.file_names, 200)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        label = []\n",
    "        filename = self.file_names[idx]\n",
    "        img = Image.open(filename)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # print(img.shape, filename)\n",
    "        source_name = filename.split('/')[-3]\n",
    "        if self.domain is None:\n",
    "            label.append(self.label_dict[source_name])\n",
    "        else:\n",
    "            if source_name == self.domain:\n",
    "                label.append(1)\n",
    "            else: label.append(0)\n",
    "        if self.one_hot:\n",
    "            label = np.array(label)\n",
    "            label = np.eye(self.n_classes)[label]\n",
    "            label = np.float32(label)\n",
    "        else:\n",
    "            label = np.array(label)\n",
    "        # sample = {'image': img, 'label': label}\n",
    "        sample = [img, label]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we used two ways to create dataloaders, they are used for class classification and domain classification tasks separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two methods are used for class classification\n",
    "def load_training(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize([256, 256]),\n",
    "         transforms.RandomCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor()])\n",
    "    data = datasets.ImageFolder(root=os.path.join(root_path, dir), transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def load_testing(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize([224, 224]),\n",
    "         transforms.ToTensor()])\n",
    "    data = datasets.ImageFolder(root=os.path.join(root_path, dir), transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two methods are used for domain classification\n",
    "def load_training(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize([256, 256]),\n",
    "         transforms.RandomCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor()])\n",
    "    data = OfficeHomeDataset(os.path.join(root_path, dir), transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def load_testing(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize([224, 224]),\n",
    "         transforms.ToTensor()])\n",
    "    data = OfficeHomeDataset(os.path.join(root_path, dir), transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model stucture\n",
    "\n",
    "In the pre-adaptation stage, we need to train a source-specific feature extractor $F_{si}$ and a source-specificclassifier $Q_{si}$ for each specific source $S_i$. For the domain, there is a domainclassifier($D_{Si}$) to be trained. In addition, in the second stage, the weighted aligned source encoder $E_{Si}$ for each source $S_i$, and the target encoder $E_T$ as well as the target classifier $Q_T$ need to be trained. The following section is going to describe each model struture in details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor and Source-specificclassifier\n",
    "\n",
    "The archtercture of the $F_{si}$ is implemented by the following layers: \n",
    "\n",
    "* ImageNet pre-trained ResNet-50 till average pool layer\n",
    "* Linear FC (2048, 1024) + ELU\n",
    "* Linear FC (1024, 1024) + BatchNorm + ELU\n",
    "* Linear FC (1024, f _dim) + ELU\n",
    "* Linear FC ( f _dim, f _dim) + BatchNorm + ELU \n",
    "\n",
    "For the source-specificclassifier $Q_{si}$, It adds a layer on the basis of feature extractor\n",
    "\n",
    "* LinearFC(f_dim,3)\n",
    "\n",
    "We used pretrained Resnet model for the first layer, and the training details for Office-Home can be found in the appendix of the original paper. Our implementaion of the Feature extrator and the source-specificclassifier can be found as follings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class SourceClassifer(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=65):\n",
    "        super(SourceClassifer, self).__init__()\n",
    "\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=False)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "\n",
    "        self.extractor1 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.extractor2 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.extractor3 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.cls1 = nn.Linear(self.f_dim, self.n_classes)\n",
    "        self.cls2 = nn.Linear(self.f_dim, self.n_classes)\n",
    "        self.cls3 = nn.Linear(self.f_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, data_src, label_src = 0, mark = 1, training=True):\n",
    "        \n",
    "        if training == True:\n",
    "            h1 = self.ResNet50(data_src)\n",
    "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "\n",
    "            if mark == 1:\n",
    "                feature1 = self.extractor1(h1)\n",
    "                pred1 = self.cls1(feature1)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred1, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 2:\n",
    "                feature2 = self.extractor2(h1)\n",
    "                pred2 = self.cls2(feature2)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred2, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 3:\n",
    "                feature3 = self.extractor3(h1)\n",
    "                pred3 = self.cls3(feature3)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred3, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "        else:\n",
    "            h1 = self.ResNet50(data_src)\n",
    "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "\n",
    "            feature1 = self.extractor1(h1)\n",
    "            pred1 = self.cls1(feature1)\n",
    "\n",
    "            feature2 = self.extractor2(h1)\n",
    "            pred2 = self.cls2(feature2)\n",
    "\n",
    "            feature3 = self.extractor3(h1)\n",
    "            pred3 = self.cls3(feature3)\n",
    "\n",
    "            return pred1, pred2, pred3, feature1, feature2, feature3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain classifier\n",
    "\n",
    "The architecture of domain classifier adds a extra layer on the architecture of the feature extractor:\n",
    "\n",
    "*  Linear FC (f_dim, f_dim/2) + ELU + Linear FC (f_dim/2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, sourceClassifier, f_dim=256, n_classes=2):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.half_f_dim = self.f_dim // 2\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.sourceClassifier = sourceClassifier\n",
    "\n",
    "        self.domain_cls1 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "        self.domain_cls2 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "        self.domain_cls3 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data_src, data_tgt=None, label_src=0, label_tgt=0, mark=1, training=True):\n",
    "\n",
    "        if training == True:\n",
    "            _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
    "            _, _, _, feature1_tgt, feature2_tgt, feature3_tgt = self.sourceClassifier(data_tgt, training=False)\n",
    "\n",
    "            if mark == 1:\n",
    "                logits1 = self.domain_cls1(feature1)\n",
    "                logits1_tgt = self.domain_cls1(feature1_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits1, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits1_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 2:\n",
    "                logits2 = self.domain_cls2(feature2)\n",
    "                logits2_tgt = self.domain_cls2(feature2_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits2, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits2_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 3:\n",
    "                logits3 = self.domain_cls1(feature3)\n",
    "                logits3_tgt = self.domain_cls1(feature3_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits3, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits3_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "        else:\n",
    "            _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
    "\n",
    "            logits1 = self.domain_cls1(feature1)\n",
    "            logits2 = self.domain_cls2(feature2)\n",
    "            logits3 = self.domain_cls3(feature3)\n",
    "\n",
    "            return logits1, logits2, logits3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Source encoder\n",
    " \n",
    " The architecture of Source encoder is as follows:\n",
    " \n",
    " * Linear FC ( f _dim, 1024) + BatchNorm + ELU\n",
    " * Linear FC (1024, 1024) + BatchNorm + ELU\n",
    " * Linear FC (1024, c_dim) + BatchNorm + ELU\n",
    " * Linear FC (c_dim, c_dim) + BatchNorm + ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceClassifier(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=3):\n",
    "        super(SourceClassifier, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=False)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "\n",
    "        self.sourceFeatureExtractor = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        h1 = self.ResNet50(input_batch)\n",
    "        h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "        source_feature = self.sourceFeatureExtractor(h1)\n",
    "        classification = self.classifier(source_feature)\n",
    "        return source_feature, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoder \n",
    "The architecture of target encoder is as follows:\n",
    "\n",
    "* ImageNet pre-trained ResNet-50 till average pool layer\n",
    "* Linear FC (2048, 1024) + ELU\n",
    "* Linear FC (1024, 1024) + BatchNorm + ELU\n",
    "* Linear FC (1024, c_dim) + BatchNorm + ELU\n",
    "* Linear FC (c_dim, c_dim) + BatchNorm + ELU\n",
    "\n",
    "We use pretrained Resnet model for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=3):\n",
    "        super(TargetEncoder, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "        for param in self.ResNet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        h1 = self.ResNet50(input_batch)\n",
    "        h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "        feature = self.encoder(h1)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target classifier \n",
    "\n",
    "The architecture of target classifier adds a extra layer on the architecture of the target encoder:\n",
    "    \n",
    "* Linear FC (c_dim, c_dim) + ELU + Linear FC (c_dim, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Targetclassifier(nn.Module):\n",
    "    def __init__(self, sourceClassifier, f_dim=256, c_dim=256, n_classes=65):\n",
    "        super(Targetclassifier, self).__init__()\n",
    "\n",
    "        self.sourceClassifier = sourceClassifier\n",
    "        self.f_dim = f_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_src, label_src=0, mark=1, training=True, encoding=None):\n",
    "\n",
    "        if training == True:\n",
    "\n",
    "            if mark == 1:\n",
    "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature1 = self.encoder1(source_feature)\n",
    "                pred1 = self.cls(feature1)\n",
    "\n",
    "                loss = loss_qt(pred1, label_src, mark=1)\n",
    "\n",
    "                return loss\n",
    "\n",
    "            if mark == 2:\n",
    "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature2 = self.encoder2(source_feature)\n",
    "                pred2 = self.cls(feature2)\n",
    "\n",
    "                loss = loss_qt(pred2, label_src, mark=2)\n",
    "\n",
    "                return loss\n",
    "\n",
    "            if mark == 3:\n",
    "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
    "                feature3 = self.encoder3(source_feature)\n",
    "                pred3 = self.cls(feature3)\n",
    "\n",
    "                loss = loss_qt(pred3, label_src, mark=3)\n",
    "\n",
    "                return loss\n",
    "\n",
    "        else:\n",
    "            if encoding is not None:\n",
    "                pred = self.cls(encoding)\n",
    "\n",
    "            else:\n",
    "\n",
    "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature1 = self.encoder1(source_feature)\n",
    "                pred1 = self.cls(feature1)\n",
    "\n",
    "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature2 = self.encoder2(source_feature)\n",
    "                pred2 = self.cls(feature2)\n",
    "\n",
    "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
    "                feature3 = self.encoder3(source_feature)\n",
    "                pred3 = self.cls(feature3)\n",
    "\n",
    "                return pred1, pred2, pred3, feature1, feature2, feature3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step\n",
    "\n",
    "Before training, we have implemented several loss functions (binary cross-entropy loss, coral loss, qt loss, align loss, distill loss, entropy loss, de loss, and T->W loss) for different models. These losses are used for backpropagation to calculate the gradient. The models and the optimizers are said before.\n",
    "\n",
    "First, we use the dataloader to load the training data and the test data. each source has its own dataloader, and then the images are iterated. After that, our model is evaluated by the accuracy of the test set.\n",
    "\n",
    "For the training settings, the batch size is 32 and the number of epochs is 10. In addition, the learning rate is given by the paper, so we only follow the paper's learning rate settings.\n",
    "\n",
    "This procedure can be done three times, the first stage is to train the feature extractor, and then the domain classifier is trained based on the feature extractor. The last step is to train all models(target encoder, target classifier, and source encoder together by using different loss functions and optimizers. Due to space limitations, we will only show the function that we trained in the second part and loss functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_qt(output, target, mark=1, relevance=alpha, n_classes=3):\n",
    "    label = target[0]\n",
    "    weight = relevance[mark-1] / output.shape[0]\n",
    "\n",
    "    loss = F.cross_entropy(output, target) * weight\n",
    "    return loss\n",
    "\n",
    "def CORAL(source, target):\n",
    "    d = source.data.shape[1]\n",
    "\n",
    "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
    "    xct = xmt.t() @ xmt\n",
    "    \n",
    "    xm = torch.mean(source, 0, keepdim=True) - source\n",
    "    xc = xm.t() @ xm\n",
    "\n",
    "    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n",
    "    loss = loss/(4*d*d)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_align(source1_output, source2_output, source3_output, target_output, K=3, alpha=alpha, beta=beta):\n",
    "    loss = 0.0\n",
    "\n",
    "    loss += alpha[0] * CORAL(source1_output, target_output)\n",
    "    loss += alpha[1] * CORAL(source2_output, target_output)\n",
    "    loss += alpha[2] * CORAL(source3_output, target_output)\n",
    "\n",
    "    loss += beta[\"0-1\"] * CORAL(source1_output, source2_output) / (K-1)\n",
    "    loss += beta[\"0-2\"] * CORAL(source1_output, source3_output) / (K-1)\n",
    "    loss += beta[\"1-2\"] * CORAL(source2_output, source3_output) / (K-1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_tw(es, et, w, ind, mark=1):\n",
    "    res = 0.0\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    res = torch.sum(w * loss(et, es))\n",
    "\n",
    "    return res\n",
    "\n",
    "def loss_distill(w1, w2, w3, pred1, pred2, pred3, pred):\n",
    "    loss = nn.L1Loss()\n",
    "\n",
    "    res = 0.0\n",
    "    w1 = w1.reshape(-1, 1)\n",
    "    w2 = w2.reshape(-1, 1)\n",
    "    w3 = w3.reshape(-1, 1)\n",
    "\n",
    "    phi = w1 * pred1 + w2 * pred2 + w3 * pred3\n",
    "\n",
    "    res = loss(pred, phi)\n",
    "\n",
    "    return res\n",
    "\n",
    "def loss_entropy(pred):\n",
    "    res = 0.0\n",
    "    p = F.softmax(pred, dim=-1)\n",
    "    res = -1 * torch.sum(p * F.log_softmax(pred, dim=-1)) / pred.size()[0]\n",
    "\n",
    "    return res\n",
    "\n",
    "def loss_de(iteration, distill, entropy, m=0.0036):\n",
    "    res = 0.0\n",
    "    mu = min(1, m*iteration)\n",
    "\n",
    "    res = (1-mu) * distill + mu * entropy\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = len(source1_weight)\n",
    "print(\"iteration: \", iteration)\n",
    "epoch = 15\n",
    "cuda = True\n",
    "seed = 8\n",
    "log_interval = 20\n",
    "class_num = 65\n",
    "batch_size = 32\n",
    "root_path = \"./Dataset/\"\n",
    "source1_name = \"Art\"\n",
    "source2_name = 'Clipart'\n",
    "source3_name = 'Product'\n",
    "target_name = \"Real World\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "target_test_loader = load_testing(root_path, target_name, batch_size, kwargs)\n",
    "\n",
    "def train(model1, model2, domainclassifier):\n",
    "\n",
    "    # target_test_loader = load_training(root_path, target_name, batch_size, kwargs)\n",
    "    target_iter = iter(target_test_loader)  \n",
    "\n",
    "    for param in model1.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for i in range(1, iteration + 1):\n",
    "        model1.eval()\n",
    "        model2.train()         \n",
    "        LEARNING_RATE = 2e-4\n",
    "\n",
    "        # optimizer of target encoder\n",
    "        optimizer2 = torch.optim.Adam([\n",
    "            {'params': model2.encoder.parameters(), 'lr': LEARNING_RATE}\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            target_data, __ = target_iter.next()\n",
    "        except Exception as err:\n",
    "            target_iter = iter(target_test_loader)\n",
    "            target_data, __ = target_iter.next()\n",
    "\n",
    "        target_data = target_data.to(device)\n",
    "        target_data = Variable(target_data)\n",
    "\n",
    "        pred1, pred2, pred3 = domainclassifier(target_data, training=False)\n",
    "        sc_pred1, sc_pred2, sc_pred3, _, _, _ = sourceClassifier(target_data, training=False)\n",
    "\n",
    "        logits1 = softmax(pred1)\n",
    "        logits2 = softmax(pred2)\n",
    "        logits3 = softmax(pred3)\n",
    "\n",
    "        weights_sum = logits1[:, 1] * alpha[0] + logits2[:, 1] * alpha[1] + logits3[:, 1] * alpha[2]\n",
    "\n",
    "        w1 = logits1[:, 1] * alpha[0] / weights_sum\n",
    "        w2 = logits2[:, 1] * alpha[1] / weights_sum\n",
    "        w3 = logits3[:, 1] * alpha[2] / weights_sum\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        target_feature = model2(target_data)\n",
    "        pred_target = model1(None, training=False, encoding=target_feature)\n",
    "\n",
    "        _, _, _, feature1, feature2, feature3 = model1(target_data, training=False)\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        loss_value_tw = loss_tw(feature1, target_feature, w1, i-1, mark=1)\n",
    "        loss_value_tw += loss_tw(feature2, target_feature, w2, i-1, mark=2)\n",
    "        loss_value_tw += loss_tw(feature3, target_feature, w3, i-1, mark=3)\n",
    "        loss_value_tw /= 3\n",
    "\n",
    "        loss_value_distill = loss_distill(w1, w2, w3, sc_pred1, sc_pred2, sc_pred3, pred_target)\n",
    "        loss_value_entropy = loss_entropy(pred_target)\n",
    "\n",
    "        loss_value_de = loss_de(i, loss_value_distill, loss_value_entropy, m=0.01)\n",
    "\n",
    "        loss = loss_value_tw + loss_value_de\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "              print('Train target iter: {} [({:.0f}%)]\\tT->WLoss: {:.6f}\\tLoss_distill: {:.6f}\\tLoss_entropy: {:.6f}\\tLoss_de: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                  i, 100. * i / iteration, loss_value_tw.item(), loss_value_distill.item(), loss_value_entropy.item(), loss_value_de.item(), loss.item()))\n",
    "    \n",
    "    return model1, model2\n",
    "\n",
    "def test(mode11, model2):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in target_test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.reshape(-1).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            feature = model2(data)\n",
    "\n",
    "            pred = model1(None, training=False, encoding=feature)\n",
    "            pred = pred.data.max(1)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        print(target_name, '\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct, len(target_test_loader.dataset),\n",
    "            100. * correct / len(target_test_loader.dataset)))\n",
    "        print('\\nsource1 accnum {}, source2 accnum {}，source3 accnum {}'.format(correct1, correct2, correct3))\n",
    "\n",
    "    return correct\n",
    "                  \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We have done four types of experiments, For the no adapt settings, we have built a ResNet classifier. First, we used the pre-trained ResNet model and added a layer to do classification. Surprisingly, the result converges slower than we thought. It gained a good result after 10 epochs. Then, we also built the proposed approach to do a no adapt domain classifier, and There is only 4.4% difference from the result in the paper.We think this small difference may caused by the batch size or the number of epoches. Besides, the effect of the proposed approach is better than the ResNet. For the second experiment, we find the average accurace obtained by us only 2.7% smaller than the paper result, and the effectness can be seen from the comparison with MFSAN result.  \n",
    "\n",
    "\n",
    "|Standard    |Method       |CPR->A|APR->C|ACR->P|ACP->R|Avg   |\n",
    "|  ----      | ----        | ---- | ---- |----  |----  |----  |\n",
    "|No Adapt by paper  |ResNet       |65.3%|49.6%|79.7%|75.4%|67.5%|\n",
    "|No Adapt by us   |ResNet         |61.9%|44.7%|73.5%|72.1%|63.1%|\n",
    "|No Adapt by paper   |Ours          |65.6%|53.8%|78.6%|73.2%|67.8%  |\n",
    "|No Adapt by us   |Ours             |64.1% |57.7%|76.3%| |0.7153  |\n",
    "|Multi-Source by paper|MFSAN        |72.1%|62.0%|80.3%|81.8%|74.1%|\n",
    "|Multi-Source by us|MFSAN        |67.7%|61.3%|77.1%|79.6%|71.4%|\n",
    "|Multi-Source by paper|Ours         |71.9%|61.4%|84.1%|82.3%|74.9%|\n",
    "|Multi-Source by us|Ours         |70.1%|59.8%|80.6%|78.2%|72.2%|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have implemented the proposed method by pytorch and make comparison with ResNet and MFSAN on **OfficeHomeDataset**. The result proved effectiveness of WAMDA. The multi-source domain adaptation based on source-source and source-target similaritiess gained better accuracy on two sub tasks than the MFSAN method. We hope our work can help people to gain a deeper understanding about multi-source domain adapation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task division\n",
    "\n",
    "The reproduce work is done by Jiaming Xu and Siwei Wang, we follow the criteria of Full re-implementation. We have met every sunday since 28 March and discuss the project together.\n",
    "\n",
    "Jiaming is responsible for coding the feature extractor, domain classifier and the sturcture of the second stage. In addition, Jiaming also did each experiment on one combination of the datasets. Last but not the least, Jiaming finished part of the report writting.\n",
    "\n",
    "Siwei is responsible for coding the ResNet model and MFSAN model. Besides, Siwei did experiments on remaining three combination of the datasets. Finally, Siwei finished part of the report writting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

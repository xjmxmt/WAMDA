{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Project: WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 10 Jiaming XU and Siwei Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the learning goals for Deep Learning course is to reproduce a paper given by the course instructor. This blog gives clear information about our reproduce work for the paper WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation from Surbhi Aggarwal, Jogendra Nath Kundu, R. Venkatesh Babu and Anirban Chakraborty. Our reproduction is based on original paper, also some other papers related to source Domain Adaptation，and online resources.\n",
    "\n",
    "The paper presents a novel method for Multi-source Domain Adaptation named WAMDA which uses multiple sourece based on their internal relevance and their relavence score related to the target. Our work is to reproduce the proposed approach on only one dataset **OfficeHome dataset** and other two methods(Resnet and MFSAN for evaluate the effectiveness of the proposed method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sturcture\n",
    "\n",
    "WAMDA is a method which can do effective multi-source domain adaptation based on the the source-target and source-source similarities. The following gives information about the basic structure.\n",
    "\n",
    "The proposed algorithm is divided into two parts. The first stage is *pre-adaptation training* , we can obtain the relevance score, feature extractor, source classifier and domain classfier from this stage. Then, the other stage is *multi-source adaptation training* , the weighted alignment of domains are performed and a classifier is learnt based on this weighted aligned space. The basic model stucture is shown as follows: ![avatar](imgs/process.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "The original paper did experiments on three datasets(*Office-31, Office-Caltech and Office-Home*). In addition, it uses four types of baseline(*No Adapt, Single-Source Best,Single-Source Best and Multi-Source* to analyse the performance of MSDA methods. For our experiment, we only did experiment on *office-Home* dataset and there are only two types of baseline we have implemented: (1) *No Adapt*: Resnet and the Proposed method (2)*Multi-Source*: MFSAN and the Proposed method. Last but not the least, the implementation steps is just the same as the original paper and will be described in the next section. Due to time limitation, we have only implement the first row and the third row of table 5 for **OfficeHomeDataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementaion\n",
    "All the experiments were done based on Google Colab, and the framework we used is pytorch. This method is achieved by these steps. Fisrt, the dataset was downloaded and transfered into certain format. Then，we trained a feature extractor source classifier and domain classifer based on the datasets. After that, we extract the relevance scores from the last step, and scores will be used in following steps. here, we also trained a weighted aligned source encoder and the target encoder.\n",
    "\n",
    "The following libaries are used in onr experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We export the *Office-home* dataset to Google Cloud Disk and decompress it. The Office-Home dataset has been created to evaluate domain adaptation algorithms for object recognition using deep learning. It consists of images from 4 different domains: Artistic images, Clip Art, Product images and Real-World images. For each domain, the dataset contains images of 65 object categories found typically in Office and Home settings. The folowing is going to show the eg from the dataset. ![avatar](imgs/dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write a class to load the data and assign each type of data a different label. In addtion, we also implement the one-hot, transformer, balance setting which can be used according to different model requrimrnts. The following code can show the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfficeHomeDataset(Dataset):\n",
    "    def __init__(self, data_path, domain=\"Real World\", balance=False, one_hot=False, transform=None):\n",
    "        self.transform = transform\n",
    "        self.domain = domain\n",
    "        self.balance = balance\n",
    "        self.one_hot = one_hot\n",
    "\n",
    "        # label dict\n",
    "        self.label_dict = {\"Art\": 0, \"Clipart\":1, \"Product\":2, 'Real World': 3}\n",
    "\n",
    "        # Read all file names\n",
    "        self.file_names = []\n",
    "        if self.domain is None:\n",
    "            self.n_classes = 3\n",
    "            for root, dirs, files in os.walk(data_path):\n",
    "                for filename in files:\n",
    "                    if filename == \".DS_Store\": continue\n",
    "                    elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                    self.file_names.append(os.path.join(root, filename))\n",
    "        else:\n",
    "            self.n_classes = 2\n",
    "            domain_file = []\n",
    "            source_file = []\n",
    "            for root, dirs, files in os.walk(data_path):\n",
    "                if self.domain in root:\n",
    "                    for filename in files:\n",
    "                        if filename == \".DS_Store\": continue\n",
    "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                        domain_file.append(os.path.join(root, filename))\n",
    "                else:\n",
    "                    for filename in files:\n",
    "                        if filename == \".DS_Store\": continue\n",
    "                        elif os.path.splitext(filename)[-1] == \".txt\": continue\n",
    "                        source_file.append(os.path.join(root, filename))\n",
    "            if balance:\n",
    "                self.file_names = domain_file + sample(source_file, len(domain_file))\n",
    "            else:\n",
    "                self.file_names = domain_file + source_file\n",
    "        \n",
    "        print(len(self.file_names))\n",
    "        # self.file_names = sample(self.file_names, 200)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        label = []\n",
    "        filename = self.file_names[idx]\n",
    "        img = Image.open(filename)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # print(img.shape, filename)\n",
    "        source_name = filename.split('/')[-3]\n",
    "        if self.domain is None:\n",
    "            label.append(self.label_dict[source_name])\n",
    "        else:\n",
    "            if source_name == self.domain:\n",
    "                label.append(1)\n",
    "            else: label.append(0)\n",
    "        if self.one_hot:\n",
    "            label = np.array(label)\n",
    "            label = np.eye(self.n_classes)[label]\n",
    "            label = np.float32(label)\n",
    "        else:\n",
    "            label = np.array(label)\n",
    "        # sample = {'image': img, 'label': label}\n",
    "        sample = [img, label]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model stucture\n",
    "\n",
    "In the pre-adaptation stage, we need to train a source-specific feature extractor $F_{si}$ and a source-specificclassifier $Q_{si}$ for each specific source $S_i$. For the domain, there is a domainclassifier($D_{Si}$) to be trained. In addition, in the second stage, the weighted aligned source encoder $E_{Si}$ for each source $S_i$, and the target encoder $E_T$ as well as the target classifier $Q_T$ need to be trained. The following section is going to describe each model struture in details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor and Source-specificclassifier\n",
    "\n",
    "The archtercture of the $F_{si}$ is implemented by the following layers: \n",
    "\n",
    "* ImageNet pre-trained ResNet-50 till average pool layer\n",
    "* Linear FC (2048, 1024) + ELU\n",
    "* Linear FC (1024, 1024) + BatchNorm + ELU\n",
    "* Linear FC (1024, f _dim) + ELU\n",
    "* Linear FC ( f _dim, f _dim) + BatchNorm + ELU \n",
    "\n",
    "For the source-specificclassifier $Q_{si}$, It adds a layer on the basis of feature extractor\n",
    "\n",
    "* LinearFC(f_dim,3)\n",
    "\n",
    "We used pretrained Resnet model for the first layer, and the training details for Office-Home can be found in the appendix of the original paper. Our implementaion of the Feature extrator and the source-specificclassifier can be found as follings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class SourceClassifer(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=65):\n",
    "        super(SourceClassifer, self).__init__()\n",
    "\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=False)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "\n",
    "        self.extractor1 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.extractor2 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.extractor3 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.cls1 = nn.Linear(self.f_dim, self.n_classes)\n",
    "        self.cls2 = nn.Linear(self.f_dim, self.n_classes)\n",
    "        self.cls3 = nn.Linear(self.f_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, data_src, label_src = 0, mark = 1, training=True):\n",
    "        \n",
    "        if training == True:\n",
    "            h1 = self.ResNet50(data_src)\n",
    "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "\n",
    "            if mark == 1:\n",
    "                feature1 = self.extractor1(h1)\n",
    "                pred1 = self.cls1(feature1)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred1, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 2:\n",
    "                feature2 = self.extractor2(h1)\n",
    "                pred2 = self.cls2(feature2)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred2, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 3:\n",
    "                feature3 = self.extractor3(h1)\n",
    "                pred3 = self.cls3(feature3)\n",
    "\n",
    "                cls_loss = F.cross_entropy(pred3, label_src)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "        else:\n",
    "            h1 = self.ResNet50(data_src)\n",
    "            h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "\n",
    "            feature1 = self.extractor1(h1)\n",
    "            pred1 = self.cls1(feature1)\n",
    "\n",
    "            feature2 = self.extractor2(h1)\n",
    "            pred2 = self.cls2(feature2)\n",
    "\n",
    "            feature3 = self.extractor3(h1)\n",
    "            pred3 = self.cls3(feature3)\n",
    "\n",
    "            return pred1, pred2, pred3, feature1, feature2, feature3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain classifier\n",
    "\n",
    "The architecture of domain classifier adds a extra layer on the architecture of the feature extractor:\n",
    "\n",
    "*  Linear FC (f_dim, f_dim/2) + ELU + Linear FC (f_dim/2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, sourceClassifier, f_dim=256, n_classes=2):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.half_f_dim = self.f_dim // 2\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.sourceClassifier = sourceClassifier\n",
    "\n",
    "        self.domain_cls1 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "        self.domain_cls2 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "        self.domain_cls3 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.half_f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.half_f_dim, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data_src, data_tgt=None, label_src=0, label_tgt=0, mark=1, training=True):\n",
    "\n",
    "        if training == True:\n",
    "            _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
    "            _, _, _, feature1_tgt, feature2_tgt, feature3_tgt = self.sourceClassifier(data_tgt, training=False)\n",
    "\n",
    "            if mark == 1:\n",
    "                logits1 = self.domain_cls1(feature1)\n",
    "                logits1_tgt = self.domain_cls1(feature1_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits1, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits1_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 2:\n",
    "                logits2 = self.domain_cls2(feature2)\n",
    "                logits2_tgt = self.domain_cls2(feature2_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits2, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits2_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "            if mark == 3:\n",
    "                logits3 = self.domain_cls1(feature3)\n",
    "                logits3_tgt = self.domain_cls1(feature3_tgt)\n",
    "                a = 1 / data_src.shape[0]\n",
    "                weights = torch.Tensor([a] * self.n_classes).to(device)\n",
    "\n",
    "                cls_loss = F.cross_entropy(logits3, label_src, weight=weights) \\\n",
    "                    + F.cross_entropy(logits3_tgt, label_tgt, weight=weights)\n",
    "\n",
    "                return cls_loss\n",
    "\n",
    "        else:\n",
    "            _, _, _, feature1, feature2, feature3 = self.sourceClassifier(data_src, training=False)\n",
    "\n",
    "            logits1 = self.domain_cls1(feature1)\n",
    "            logits2 = self.domain_cls2(feature2)\n",
    "            logits3 = self.domain_cls3(feature3)\n",
    "\n",
    "            return logits1, logits2, logits3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Source encoder\n",
    " \n",
    " The architecture of Source encoder is as follows:\n",
    " \n",
    " * Linear FC ( f _dim, 1024) + BatchNorm + ELU\n",
    " * Linear FC (1024, 1024) + BatchNorm + ELU\n",
    " * Linear FC (1024, c_dim) + BatchNorm + ELU\n",
    " * Linear FC (c_dim, c_dim) + BatchNorm + ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceClassifier(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=3):\n",
    "        super(SourceClassifier, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=False)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "\n",
    "        self.sourceFeatureExtractor = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        h1 = self.ResNet50(input_batch)\n",
    "        h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "        source_feature = self.sourceFeatureExtractor(h1)\n",
    "        classification = self.classifier(source_feature)\n",
    "        return source_feature, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoder \n",
    "The architecture of target encoder is as follows:\n",
    "\n",
    "* ImageNet pre-trained ResNet-50 till average pool layer\n",
    "* Linear FC (2048, 1024) + ELU\n",
    "* Linear FC (1024, 1024) + BatchNorm + ELU\n",
    "* Linear FC (1024, c_dim) + BatchNorm + ELU\n",
    "* Linear FC (c_dim, c_dim) + BatchNorm + ELU\n",
    "\n",
    "We use pretrained Resnet model for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(nn.Module):\n",
    "    def __init__(self, f_dim=256, n_classes=3):\n",
    "        super(TargetEncoder, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Get ResNet50 model\n",
    "        ResNet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "        ResNet50.fc = nn.Identity()\n",
    "        self.ResNet50 = ResNet50\n",
    "        for param in self.ResNet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.f_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.f_dim, self.f_dim),\n",
    "            nn.BatchNorm1d(self.f_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        h1 = self.ResNet50(input_batch)\n",
    "        h1 = torch.flatten(h1, start_dim=1)  # size: (batch_size, dim)\n",
    "        feature = self.encoder(h1)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target classifier \n",
    "\n",
    "The architecture of target classifier adds a extra layer on the architecture of the target encoder:\n",
    "    \n",
    "* Linear FC (c_dim, c_dim) + ELU + Linear FC (c_dim, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Targetclassifier(nn.Module):\n",
    "    def __init__(self, sourceClassifier, f_dim=256, c_dim=256, n_classes=65):\n",
    "        super(Targetclassifier, self).__init__()\n",
    "\n",
    "        self.sourceClassifier = sourceClassifier\n",
    "        self.f_dim = f_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(self.f_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),  # expect 2-D input\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.BatchNorm1d(self.c_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.c_dim, self.c_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.c_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_src, label_src=0, mark=1, training=True, encoding=None):\n",
    "\n",
    "        if training == True:\n",
    "\n",
    "            if mark == 1:\n",
    "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature1 = self.encoder1(source_feature)\n",
    "                pred1 = self.cls(feature1)\n",
    "\n",
    "                loss = loss_qt(pred1, label_src, mark=1)\n",
    "\n",
    "                return loss\n",
    "\n",
    "            if mark == 2:\n",
    "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature2 = self.encoder2(source_feature)\n",
    "                pred2 = self.cls(feature2)\n",
    "\n",
    "                loss = loss_qt(pred2, label_src, mark=2)\n",
    "\n",
    "                return loss\n",
    "\n",
    "            if mark == 3:\n",
    "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
    "                feature3 = self.encoder3(source_feature)\n",
    "                pred3 = self.cls(feature3)\n",
    "\n",
    "                loss = loss_qt(pred3, label_src, mark=3)\n",
    "\n",
    "                return loss\n",
    "\n",
    "        else:\n",
    "            if encoding is not None:\n",
    "                pred = self.cls(encoding)\n",
    "\n",
    "            else:\n",
    "\n",
    "                _, _, _, source_feature, _, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature1 = self.encoder1(source_feature)\n",
    "                pred1 = self.cls(feature1)\n",
    "\n",
    "                _, _, _, _, source_feature, _ = self.sourceClassifier(data_src, training=False)\n",
    "                feature2 = self.encoder2(source_feature)\n",
    "                pred2 = self.cls(feature2)\n",
    "\n",
    "                _, _, _, _, _, source_feature = self.sourceClassifier(data_src, training=False)\n",
    "                feature3 = self.encoder3(source_feature)\n",
    "                pred3 = self.cls(feature3)\n",
    "\n",
    "                return pred1, pred2, pred3, feature1, feature2, feature3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step\n",
    "\n",
    "Before training, we have implemented three loss functions(binary cross entropy loss, coral loss, qt loss) for different output. These losses are used by backpropagation to calculate the graidient. The models and the optimizers are talken before.\n",
    "\n",
    "First, we use the dataloader to load the training data and the test data. each source has its own dataloader, and then the images are iterated. After that, our model is evaluated by accuracy of test set.\n",
    "\n",
    "For the training settings,  the batchsize is 32 and the number of epoch is 10. In addition, the learning rate is given by the paper, so we only follow the paper's learning rate settings.\n",
    "\n",
    "This procedure can be done three times, the first stage is to train the feature extractor and then the domain classifier is trained based on the feature extractore. The  last step is to train all models(targer encoder, target classifier and source encoder together by using different loss function and optimizers. Due to space limitations, we will only show the function that we trained in the second part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model1, model2, path, target_path, criterion1, criterion2, optimizer1, optimizer2, num_epochs=5, batch_size=16, log_interval=40, binary_class=False, train=0.7, val=0.2, test=0.1):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts1 = copy.deepcopy(model1.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    l = os.listdir(path)\n",
    "    source_paths = list(map(lambda x: os.path.join(path, x), l))\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        source1_loader, source1_length = get_train_dataloader(source_paths[0], batch_size=batch_size)\n",
    "        source2_loader, source2_length = get_train_dataloader(source_paths[1], batch_size=batch_size)\n",
    "        source3_loader, source3_length = get_train_dataloader(source_paths[2], batch_size=batch_size)\n",
    "        target_loader, _ = get_train_dataloader(target_path, batch_size=batch_size)\n",
    "\n",
    "        source_test_loader, source_test_length = get_test_dataloader(path, batch_size=batch_size)\n",
    "\n",
    "        iterations = max(source1_length, source2_length, source3_length) // batch_size\n",
    "        print(\"iterations: \", iterations)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model1.train()\n",
    "                model2.train()\n",
    "            else:\n",
    "                model1.eval()   # Only model1 need validation phase\n",
    "            if phase == 'train':\n",
    "                running_loss1 = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "            running_loss2 = 0.0\n",
    "\n",
    "            source1_iter = iter(source1_loader)\n",
    "            source2_iter = iter(source2_loader)\n",
    "            source3_iter = iter(source3_loader)\n",
    "            target_iter = iter(target_loader)\n",
    "\n",
    "            for i in range(1, iterations + 1):\n",
    "\n",
    "          # Target domain\n",
    "              try:\n",
    "                target_data, __ = target_iter.next()\n",
    "            except Exception as err:\n",
    "                target_iter = iter(target_loader)\n",
    "                target_data, __ = target_iter.next()\n",
    "            target_data = target_data.detach().to(device)\n",
    "            target_data = Variable(target_data)\n",
    "\n",
    "          # Source domain 1\n",
    "            try:\n",
    "                source_data, source_label = source1_iter.next()\n",
    "            except Exception as err:\n",
    "                source1_iter = iter(source1_loader)\n",
    "                source_data, source_label = source1_iter.next()\n",
    "          \n",
    "              source_data, source_label = source_data.to(device), source_label.reshape(-1).to(device)\n",
    "              source_data, source_label = Variable(source_data), Variable(source_label）\n",
    "            optimizer1.zero_grad()\n",
    "\n",
    "              _, logits = model1(source_data)\n",
    "\n",
    "              # Classifier result\n",
    "              _, preds = torch.max(logits, 1)\n",
    "              # print(\"preds: \", preds, \", labels: \", source1_label)\n",
    "\n",
    "              if binary_class:\n",
    "                running_corrects += torch.sum(preds == torch.max(source_label.data, 1)[1])\n",
    "              else:\n",
    "                running_corrects += torch.sum(preds == source_label.data)\n",
    "\n",
    "              loss1 = criterion1(logits, source_label)\n",
    "              loss1.backward()\n",
    "              optimizer1.step()\n",
    "\n",
    "              if i % log_interval == 0:\n",
    "                print('Train source1 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\t'.format(\n",
    "                  i, 100. * i / iterations, loss1.item()))\n",
    "          \n",
    "              # trainging of target encoder\n",
    "              feature, _ = model1(source_data)\n",
    "\n",
    "              optimizer2.zero_grad()\n",
    "              target_feature = model2(target_data)\n",
    "              loss2 = criterion2(feature, target_feature, 1)\n",
    "              loss2.backward()\n",
    "              optimizer2.step()\n",
    "\n",
    "              epoch_loss += loss2.item()\n",
    "\n",
    "              # Source domain 2\n",
    "              try:\n",
    "                source_data, source_label = source2_iter.next()\n",
    "              except Exception as err:\n",
    "                source2_iter = iter(source2_loader)\n",
    "                source_data, source_label = source2_iter.next()\n",
    "          \n",
    "              source_data, source_label = source_data.to(device), source_label.reshape(-1).to(device)\n",
    "              source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "              optimizer1.zero_grad()\n",
    "\n",
    "              _, logits = model1(source_data)\n",
    "\n",
    "               # Classifier result\n",
    "              _, preds = torch.max(logits, 1)\n",
    "              # print(\"preds: \", preds, \", labels: \", source2_label)\n",
    "\n",
    "            if binary_class:\n",
    "            # print(torch.sum(preds == torch.max(source2_label.data, 1)[1]))\n",
    "                running_corrects += torch.sum(preds == torch.max(source_label.data, 1)[1])\n",
    "            else:\n",
    "            # print(torch.sum(preds == source2_label.data))\n",
    "                running_corrects += torch.sum(preds == source_label.data)\n",
    "\n",
    "              loss1 = criterion1(logits, source_label)\n",
    "              loss1.backward()\n",
    "              optimizer1.step()\n",
    "\n",
    "              if i % log_interval == 0:\n",
    "                print('Train source2 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\t'.format(\n",
    "                  i, 100. * i / iterations, loss1.item()))\n",
    "            \n",
    "              # trainging of target encoder\n",
    "              feature, _ = model1(source_data)\n",
    "\n",
    "              optimizer2.zero_grad()\n",
    "              target_feature = model2(target_data)\n",
    "              loss2 = criterion2(feature, target_feature, 2)\n",
    "              loss2.backward()\n",
    "              optimizer2.step()\n",
    "\n",
    "              epoch_loss += loss2.item()\n",
    "    \n",
    "              # Source domain 3\n",
    "              try:\n",
    "                source_data, source_label = source3_iter.next()\n",
    "              except Exception as err:\n",
    "                source3_iter = iter(source3_loader)\n",
    "                source_data, source_label = source3_iter.next()\n",
    "          \n",
    "                source_data, source_label = source_data.to(device), source_label.reshape(-1).to(device)\n",
    "              source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "              optimizer1.zero_grad()\n",
    "\n",
    "              _, logits = model1(source_data)\n",
    "\n",
    "              # Classifier result\n",
    "              _, preds = torch.max(logits, 1)\n",
    "              # print(\"preds: \", preds, \", labels: \", source3_label)\n",
    "\n",
    "              if binary_class:\n",
    "                running_corrects += torch.sum(preds == torch.max(source_label.data, 1)[1])\n",
    "              else:\n",
    "                running_corrects += torch.sum(preds == source_label.data)\n",
    "\n",
    "            loss1 = criterion1(logits, source_label)\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "\n",
    "              if i % log_interval == 0:\n",
    "                print('Train source3 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\t'.format(\n",
    "                  i, 100. * i / iterations, loss1.item()))\n",
    "            \n",
    "              # trainging of target encoder\n",
    "              feature, _ = model1(source_data)\n",
    "\n",
    "              optimizer2.zero_grad()\n",
    "              target_feature = model2(target_data)\n",
    "              loss2 = criterion2(feature, target_feature, 3)\n",
    "              loss2.backward()\n",
    "              optimizer2.step()\n",
    "\n",
    "              epoch_loss += loss2.item()\n",
    "\n",
    "              if i % log_interval == 0:\n",
    "                print('Train target iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\t'.format(\n",
    "                  i, 100. * i / iterations, loss2.item()))\n",
    "            \n",
    "                avg_loss = epoch_loss / (log_interval * 3)\n",
    "                if avg_loss < best_loss:\n",
    "                  best_loss = avg_loss\n",
    "                  best_model_wts2 = copy.deepcopy(model2.state_dict())\n",
    "                epoch_loss = 0.0\n",
    "            \n",
    "          elif phase == 'val':\n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0\n",
    "\n",
    "            # Val classifier\n",
    "            iterations = source_test_length // batch_size\n",
    "            for i in range(1, iterations + 1):\n",
    "              try:\n",
    "                source_data, source_label = source_test_iter.next()\n",
    "              except Exception as err:\n",
    "                source_test_iter = iter(source_test_loader)\n",
    "                source_data, source_label = source_test_iter.next()\n",
    "          \n",
    "              source_data, source_label = source_data.to(device), source_label.reshape(-1).to(device)\n",
    "              source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "\n",
    "              feature, logits = model1(source_data)\n",
    "\n",
    "              # Classifier result\n",
    "              _, preds = torch.max(logits, 1)\n",
    "              # print(\"preds: \", preds, \", labels: \", source_label)\n",
    "\n",
    "              if binary_class:\n",
    "                # print(torch.sum(preds == torch.max(source_label.data, 1)[1]))\n",
    "                val_corrects += torch.sum(preds == torch.max(source_label.data, 1)[1])\n",
    "              else:\n",
    "                # print(torch.sum(preds == source_label.data))\n",
    "                val_corrects += torch.sum(preds == source_label.data)\n",
    "\n",
    "              loss1 = criterion1(logits, source_label)\n",
    "              val_loss += loss1\n",
    "          \n",
    "            epoch_acc = val_corrects / source_test_length\n",
    "            if epoch_acc > best_acc:\n",
    "              # deep copy the model\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts1 = copy.deepcopy(model1.state_dict())\n",
    "            val_acc_history.append(epoch_acc)\n",
    "            print('Val classifier: {} [({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}\\t'.format(\n",
    "              i, 100. * i / iteration, loss2.item(), epoch_acc))\n",
    "\n",
    "            model2.load_state_dict(best_model_wts2)\n",
    "        \n",
    "      return model1, model2, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We have done four types of experiments, For the no adapt settings, we have also built a resnet classifier. First, we used the pretrained resnet model and added a layer to do classification. Surprisingly, the result converges slower than we thougt. It gained a good result after 10 epoches. Although the average of the results obtained by the Resnet is lower than that obtained with the pretrained model, it is still about 5.5% higher than that given in the paper. Then, we also built the proposed approach to do a no apapt domain clssifier, and the average of the result is 18% higher than the propsed on. In addition, the effect of the proposed approach is better than the Resnet. \n",
    "\n",
    "\n",
    "|Standard    |Method       |CPR->A|APR->C|ACR->P|ACP->R|Avg   |\n",
    "|  ----      | ----        | ---- | ---- |----  |----  |----  |\n",
    "|No Adapt by paper  |ResNet       |65.3%|49.6%|79.7%|75.4%|67.5%|\n",
    "|No Adapt by us   |ResNet         |61.9%|44.7%|73.5%|72.1%|63.1%|\n",
    "|No Adapt by paper   |Ours          |65.6%|53.8%|78.6%|73.2%|67.8%  |\n",
    "|No Adapt by us   |Ours             |64.1% |57.7%|76.3%| |0.7153  |\n",
    "|Multi-Source by paper|MFSAN        |72.1%|62.0%|80.3%|81.8%|74.1%|\n",
    "|Multi-Source by us|MFSAN        |67.7%|61.3%|77.1%|79.6%|71.4%|\n",
    "|Multi-Source by paper|Ours         |71.9%|61.4%|84.1%|82.3%|74.9%|\n",
    "|Multi-Source by us|Ours         |0.6769|0.6131|0.7709|0.7955|0.7143|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have implemented the proposed method by pytorch and make Comparison with Resnet and MFSAN on **DataHomeDataset**. The result proved effectiveness of WAMDA. The multi-source domain adaptation based on source-source and source-target similaritiess gained better accuracy on two sub tasks than the MFSAN method. We hope our work can help people to gain a deeper understanding about multi-source domain adapation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task division\n",
    "\n",
    "The reproduce work is done by Jiaming Xu and Siwei Wang. we met every sunday since 28 March and discuss the project together.\n",
    "\n",
    "Jiaming is responsible for coding the feature extractor, domain classifier and the sturcture of the second stage. In addition, Jiaming also did each experiment on one combination of the datasets. Last but not the least, Jiaming finished part of the report writting.\n",
    "\n",
    "Siwei is responsible for coding the resnet model and MFSAN model. Besides, Siwei did experiments on remaining three combination of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
